{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'XfcbXSUkdUBxbKZFRI0iwzFta'\n",
    "consumer_secret = 'JM9FM9kYfpiGkCpfw0vaeUjNoY9VKO1lCBHgFx3gJE83DQ3DB0'\n",
    "access_token = '1031510125784121344-PyoDNc5LJVhpI4q0yiOeG7WprrnAQJ'\n",
    "access_token_secret = 'mcvN9YOdGZDdPt4DwO8ZUo5AA8iAXx0D4DHTOPdxURQ1Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KeyWord = 'humalog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileNameCSV = KeyWord + '_output.csv'\n",
    "FileNameJSON = KeyWord + 'json.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Edit the Keyword \n",
    "\n",
    "last_20_tweets = api.search(KeyWord,count = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(last_20_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list_of_dicts = []\n",
    "for each_json_tweet in last_20_tweets:\n",
    "    my_list_of_dicts.append(each_json_tweet._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FileNameJSON, 'w') as file:\n",
    "        file.write(json.dumps(my_list_of_dicts, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_demo_list = []\n",
    "with open('tweet_json.txt', encoding='utf-8') as json_file:  \n",
    "    all_data = json.load(json_file)\n",
    "    for each_dictionary in all_data:\n",
    "        tweet_id = each_dictionary['id']\n",
    "        text = each_dictionary['text']\n",
    "        favorite_count = each_dictionary['favorite_count']\n",
    "        retweet_count = each_dictionary['retweet_count']\n",
    "        created_at = each_dictionary['created_at']\n",
    "        my_demo_list.append({'tweet_id': str(tweet_id),\n",
    "                             'text': str(text),\n",
    "                             'favorite_count': int(favorite_count),\n",
    "                             'retweet_count': int(retweet_count),\n",
    "                             'created_at': created_at,\n",
    "                            })\n",
    "        tweet_json = pd.DataFrame(my_demo_list, columns = \n",
    "                                  ['tweet_id', 'text', \n",
    "                                   'favorite_count', 'retweet_count', \n",
    "                                   'created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean tweet text by removing links, special characters\n",
    "    using simple regex statements.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "\n",
    "CleanText =[]\n",
    "for index, row in tweet_json.iterrows():\n",
    "    CleanText.append(clean_tweet(row['text']).lower())\n",
    "\n",
    "tweet_json[\"CleanText\"] = CleanText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify sentiment of passed tweet\n",
    "    using textblob's sentiment method\n",
    "    '''\n",
    "    # create TextBlob object of passed tweet text\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    # set sentiment\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment =[]\n",
    "for index, row in tweet_json.iterrows():\n",
    "    Sentiment.append(get_tweet_sentiment(row['CleanText']))\n",
    "\n",
    "tweet_json['Sentiment'] = Sentiment   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING\n",
    "\n",
    "\n",
    "## If we are adding external file then only run this\n",
    "\n",
    "import pandas as pd\n",
    " \n",
    "tweet_json = pd.read_csv(\"/home/ajadhav/Humalog Data _Complete (3).csv\",encoding='latin-1')\n",
    "\n",
    "tweet_json.columns = ['text','Sentiment']\n",
    "\n",
    "\n",
    "CleanText =[]\n",
    "for index, row in tweet_json.iterrows():\n",
    "    CleanText.append(clean_tweet(row['text']).lower())\n",
    "\n",
    "tweet_json[\"CleanText\"] = CleanText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive    3141\n",
       "Negative    1173\n",
       "Neutral     1007\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_json.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>CleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\" The way to know if the meter itself is okay ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>the way to know if the meter itself is okay is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" The way to know if the meter itself is okay ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>the way to know if the meter itself is okay is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\" The way to know if the meter itself is okay ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>the way to know if the meter itself is okay is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it might have \"burned out\" some of my ë_-cells...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>it might have burned out some of my cells sinc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>it might have \"burned out\" some of my ë_-cells...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>it might have burned out some of my cells sinc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text Sentiment  \\\n",
       "0  \" The way to know if the meter itself is okay ...  Positive   \n",
       "1  \" The way to know if the meter itself is okay ...   Neutral   \n",
       "2  \" The way to know if the meter itself is okay ...  Positive   \n",
       "3  it might have \"burned out\" some of my ë_-cells...  Negative   \n",
       "4  it might have \"burned out\" some of my ë_-cells...  Negative   \n",
       "\n",
       "                                           CleanText  \n",
       "0  the way to know if the meter itself is okay is...  \n",
       "1  the way to know if the meter itself is okay is...  \n",
       "2  the way to know if the meter itself is okay is...  \n",
       "3  it might have burned out some of my cells sinc...  \n",
       "4  it might have burned out some of my cells sinc...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(tweet_json, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "X_train = train.CleanText.apply(word_tokenize)\n",
    "X_test = test.CleanText.apply(word_tokenize)\n",
    "y_train = train.Sentiment\n",
    "y_test = test.Sentiment\n",
    "\n",
    "tweet_json['Tokens'] = tweet_json['CleanText'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "v = TfidfVectorizer()\n",
    "tweet_json['Tokens']=[\" \".join(review) for review in tweet_json['Tokens'].values]\n",
    "v.fit(tweet_json['Tokens'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_train = v.transform(X_train)\n",
    "\n",
    "X_test = X_test.apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_test = v.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train,y_train )\n",
    "predicted = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   Negative       0.61      0.93      0.74       400\n",
      "    Neutral       0.91      0.69      0.79       334\n",
      "   Positive       0.91      0.80      0.85      1022\n",
      "\n",
      "avg / total       0.84      0.81      0.81      1756\n",
      "\n",
      "0.809225512528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(classification_report(y_test, predicted))\n",
    "print(accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[370   5  25]\n",
      " [ 50 232  52]\n",
      " [186  17 819]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"PredictedSentiment\"] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = test[['text','Sentiment','PredictedSentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(FileNameCSV,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to run this code\n",
    "\n",
    "# First You need to add the KeyWord for which you want to do Sentiment Analysis\n",
    "# Suppose I want to search for \"Humalog then add this word into Chunk Number 3\n",
    "# Example\n",
    "# ---------------\n",
    "#  api.search('humalog',count = 100)\n",
    "#----------------\n",
    "\n",
    "# then rest of the code will pre process the text data means tweet and convert that data into Numeric form\n",
    "\n",
    "# Then we will apply ML model which is Naive Bayes\n",
    "\n",
    "# After building the model it will validate the results and create the confusion matrix and Accuracy Score\n",
    "\n",
    "# It will also save the output in csv format where three columns are there\n",
    "\n",
    "# Tweet, Actual Sentiment and Predicted Sentiment will be also there\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
